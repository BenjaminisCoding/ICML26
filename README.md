# aistat25
Discovery Dynamical System with LLMs

# ADUDLLM: Automated Discovery of Unknown Dynamics with LLMs

This project implements an iterative framework for the Automated Discovery of Unknown Dynamics (ADUD) in complex systems. It leverages a Large Language Model (LLM) to propose mechanistic ODE (Ordinary Differential Equation) models from partially observed time-series data. The proposed models are then optimized against the data, and the LLM receives performance feedback to refine its subsequent proposals, driving the discovery process.

## Project Structure

The project is organized into the following main directories and files:

```
.
├── run.py
├── noise_exp.ipynb
├── data/
│   ├── utils.py
│   ├── generate_data.py
│   └── config_*.json (e.g., config_WarfarinTMDD.json)
└── solve/
    ├── optim_config.json
    ├── method.py
    ├── utils.py
    └── generated_code/ (will be created automatically to store LLM-generated models)
```

### `run.py`

This is the main entry point for running the ADUD method. It orchestrates the entire discovery process:
- Loads dataset configuration.
- Generates and prepares synthetic time-series data, applying partial observability, noise, and sparsity.
- Initializes the `Method` class, which manages the LLM interaction and model optimization.
- Executes the iterative `run()` method of the `Method` class.
- Includes plotting code to visualize the training progress (best initial and final losses) across iterations.

### `noise_exp.ipynb`

A Jupyter notebook intended for experimenting with the robustness of the discovery method to different levels and types of noise in the observed data. (Not provided in the code, but mentioned in the folder structure).

### `data/`

This directory handles all data generation and preparation functionalities.

-   **`utils.py`**: Contains utility functions for preparing raw generated data.
    -   `prepare_data_for_method`: Applies partial observability (selecting specific variables), adds various types of noise (additive, multiplicative, log-normal), and introduces sparsity (missing values) to the generated time-series data.
-   **`generate_data.py`**: Defines base and specific dataset classes for generating synthetic time-series data from known dynamical systems.
    -   `Dataset` (Base Class): Provides a common interface for defining system dynamics (`dynamics` method) and data generation (`generate` method).
    -   `LorenzDataset`: Generates data from the 3D Lorenz chaotic system.
    -   `WarfarinTMDDDataset`: Generates data from a Target-Mediated Drug Disposition (TMDD) model relevant to pharmacokinetics.
    -   `MacroRBCDataset`: Generates data from a simplified Macro-Economic Real Business Cycle model.
-   **`config_*.json`**: Configuration files for defining parameters of the datasets (e.g., initial states, system parameters, simulation duration).

### `solve/`

This directory contains the core logic for the LLM-driven discovery method and model optimization.

-   **`optim_config.json`**: Configuration file for the optimization process (e.g., number of epochs, learning rate, optimizer type, early stopping parameters, learning rate scheduler settings).
-   **`method.py`**: Implements the `Method` class, the central component of the ADUD framework.
    -   Manages the conversation history with the LLM (Azure OpenAI).
    -   Handles LLM API calls for generating model proposals.
    -   Parses LLM responses into executable Python `torch.nn.Module` classes.
    -   Orchestrates the training of multiple proposed models using `solve.utils.train_model`.
    -   Generates structured feedback for the LLM based on model performance (loss, optimized parameters, simulability).
    -   Keeps track of the best-performing model found across all iterations.
    -   Dynamically adjusts the number of models to be generated by the LLM in later iterations.
-   **`utils.py`**: Contains utility functions for training individual ODE models.
    -   `read_config`: Parses the `optim_config.json` file.
    -   `test_ode_system_threaded`: Safely tests the initial simulability of an ODE model within a timeout, preventing crashes from unstable dynamics.
    -   `train_model`: Performs the training loop for a single `torch.nn.Module` ODE model, including:
        -   Initial condition handling for observed and latent variables.
        -   Integration with `torchdiffeq` for ODE solving.
        -   Loss calculation and backpropagation.
        -   Implementation of early stopping.
        -   Optional learning rate scheduling.
        -   Robustness checks for simulability during training.

### `generated_code/`

This directory is automatically created by the `method.py` script. It serves as a temporary storage location for the Python code generated by the LLM for each proposed model. These files are dynamically loaded by the system for instantiation and training.

## Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd ADUD
    ```
2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate # On Windows: `venv\Scripts\activate`
    ```
3.  **Install dependencies:**
    ```bash
    pip install torch torchaudio torchvision --index-url https://download.pytorch.org/whl/cpu # Or choose appropriate CUDA version if you have a GPU
    pip install numpy scipy matplotlib tqdm openai
    pip install torchdiffeq
    ```
4.  **Azure OpenAI API Key:**
    The `solve/method.py` file currently hardcodes the Azure OpenAI API key, endpoint, and deployment name. **For production or secure use, it is highly recommended to load these from environment variables.**
    ```python
    # In solve/method.py, replace hardcoded values with environment variable retrieval:
    # import os
    # api_key = os.getenv("AZURE_OPENAI_API_KEY")
    # azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    # deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
    ```
    Ensure your `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, and `AZURE_OPENAI_DEPLOYMENT_NAME` environment variables are set correctly if you switch to this method.

5.  **Configuration Files:**
    -   Ensure `data/config_WarfarinTMDD.json` (or another dataset config) exists and is correctly configured.
    -   Ensure `solve/config_optim.json` exists and defines the optimization parameters. Example content for `solve/config_optim.json`:
        ```json
        {
          "n_epochs": 2000,
          "lr": 0.01,
          "optimizer": "Adam",
          "solver": "odeint_adjoint",
          "early_stopping": {
            "patience": 50,
            "min_delta": 1e-5
          },
          "lr_scheduler": {
            "enabled": true,
            "mode": "min",
            "factor": 0.5,
            "patience": 20,
            "min_lr": 1e-6
          },
          "normalize": true
        }
        ```

## How to Run

To start the LLM-driven discovery process:

```bash
python run.py